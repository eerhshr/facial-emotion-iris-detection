# -*- coding: utf-8 -*-
"""FER - FullNotebook

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yid3f7WUROBtwNu5EXSxsgKwhFjXYC-U
"""

from google.colab import drive
drive.mount(('/content/drive'))

!ls

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/facial_expression/

import os
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from matplotlib import pyplot
from PIL import Image

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.preprocessing.image import ImageDataGenerator, load_img
from keras.layers import Conv2D, Dense, BatchNormalization, Activation, Dropout, MaxPooling2D, Flatten
from keras.optimizers import Adam, RMSprop, SGD
from keras import regularizers

"""## Separating data"""

data_org = pd.read_csv("fer2013/fer2013.csv")
data_org.head()

data_org.emotion.unique()

data_org.groupby(['Usage']).count()

for i, data in data_org.groupby('Usage'):
    data.to_csv("{}.csv".format(i))

test_private = pd.read_csv("PrivateTest.csv")
test_private

test_public = pd.read_csv("PublicTest.csv")
test_public

train = pd.read_csv("Training.csv")
train

"""## Emotion Distribution"""

emotion_label = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}

import seaborn as sns
sns.set_theme(style="darkgrid")
ax1 = sns.countplot(x="emotion", data=train, palette= "Set3")
ax1.set_title("Training Data Emotion Distribution");
ax1.set_xticklabels(emotion_label.values())

sns.set_theme(style="darkgrid")
ax2 = sns.countplot(x="emotion", data=test_public, palette="Set2")
ax2.set_title("Test Data Emotion Distribution");
ax2.set_xticklabels(emotion_label.values())

sns.set_theme(style="darkgrid")
ax3 = sns.countplot(x="emotion", data=test_private, palette="Set1")
ax3.set_title("Validation Data Emotion Distribution");
ax3.set_xticklabels(emotion_label.values())

"""### Separating Training Data based on Emotions"""

emotion_split = train.groupby(['emotion']).count()
emotion_split

for j, dx in train.groupby('emotion'):
    dx.to_csv("{}.csv".format(j))

#sample
disgust = pd.read_csv("train/1.csv")
disgust.head()

"""### Separating Testing Data based on Emotions"""

for i, data in test_public.groupby('emotion'):
    data.to_csv("{}.csv".format(i))

#sample
happy = pd.read_csv("test/3.csv")
happy.head()

"""### Separating Validation data based on emotions"""

for i, data in test_private.groupby('emotion'):
    data.to_csv("{}.csv".format(i))

#sample
neutral = pd.read_csv("validation/6.csv")
neutral.head()

"""#Creating Images for available data emotions

## Converting pixel information to image
"""

def pixel_to_img(path, data_df):
  tosave_path = path
  data = data_df
  for i in range(len(data)):
    x = np.array(data.pixels[i].split(" ")).reshape(48,48).astype('float')
    #pyplot.figure()
    #pyplot.imshow(x)
    fname = path.strip('/')
    filename = fname+str(i)+".jpg"
    pyplot.imsave(path+filename, x)

"""#### Conversion for Train Data"""

# Commented out IPython magic to ensure Python compatibility.
# %cd train/

train_angry = pd.read_csv('0.csv')
pixel_to_img('angry/',train_angry)

train_disgust = pd.read_csv('1.csv')
pixel_to_img('disgust/',train_disgust)

train_fear = pd.read_csv('2.csv')
pixel_to_img('fear/',train_fear)

train_happy = pd.read_csv('3.csv')
pixel_to_img('happy/',train_happy)

train_sad = pd.read_csv('4.csv')
pixel_to_img('sad/',train_sad)

train_suprise = pd.read_csv('5.csv')
pixel_to_img('suprise/',train_suprise)

train_neutral = pd.read_csv('6.csv')
pixel_to_img('neutral/',train_neutral)

"""#### Conversion for Test Data

"""

test_angry = pd.read_csv('0.csv')
pixel_to_img('angry/',test_angry)

test_disgust = pd.read_csv('1.csv')
pixel_to_img('disgust/',test_disgust)

test_fear = pd.read_csv('2.csv')
pixel_to_img('fear/',test_fear)

test_happy = pd.read_csv('3.csv')
pixel_to_img('happy/',test_happy)

test_sad = pd.read_csv('4.csv')
pixel_to_img('sad/',test_sad)

test_suprise = pd.read_csv('5.csv')
pixel_to_img('suprise/',test_suprise)

test_neutral = pd.read_csv('6.csv')
pixel_to_img('neutral/',test_neutral)

"""#### Conversion for Validation data"""

v_angry = pd.read_csv('0.csv')
pixel_to_img('angry/',v_angry)

v_disgust = pd.read_csv('1.csv')
pixel_to_img('disgust/',v_disgust)

v_fear = pd.read_csv('2.csv')
pixel_to_img('fear/',v_fear)

v_happy = pd.read_csv('3.csv')
pixel_to_img('happy/',v_happy)

v_sad = pd.read_csv('4.csv')
pixel_to_img('sad/',v_sad)

v_suprise = pd.read_csv('5.csv')
pixel_to_img('suprise/',v_suprise)

v_neutral = pd.read_csv('6.csv')
pixel_to_img('neutral/',v_neutral)

"""## Converting Images to Gray Scale Images - Batch Conversion"""

direc = os.listdir(input_folder)
for i in direc:
    img = Image.open(input_folder+i)
    grayimg = img.convert('L')
    grayimg.save(output_folder+i)

"""#### Path for Data - GrayScale"""

input_folder = './angry/'
output_folder = './angry_BW/'

input_folder = './disgust/'
output_folder = './disgust_BW/'

input_folder = './fear/'
output_folder = './fear_BW/'

input_folder = './happy/'
output_folder = './happy_BW/'

input_folder = './sad/'
output_folder = './sad_BW/'

input_folder = './suprise/'
output_folder = './suprise_BW/'

input_folder = './neutral/'
output_folder = './neutral_BW/'

"""#**Building a two class classification model**

### Helper function to analyze number of images in each folder
"""

def count_exp(path, name):
    d = {}
    for emotion in os.listdir(path):
        direc = path + emotion
        d[emotion] = len(os.listdir(direc))
    df = pd.DataFrame(d, index=[name])
    return df

train_path = '/content/drive/MyDrive/facial_expression/data/train/'
test_path = '/content/drive/MyDrive/facial_expression/data/test/'

train_img_count = count_exp(train_path, 'train')
test_img_count = count_exp(test_path, 'test')

print(train_img_count)
print(test_img_count)

"""### Visualizing Images in Train and Test directories"""

plt.figure(figsize=(14,22))
i = 1
for e in os.listdir(train_path):
    img = load_img((train_path + e +'/'+ os.listdir(train_path + e)[5]))
    plt.subplot(1,7,i)
    plt.imshow(img)
    plt.title(e)
    plt.axis('off')
    i += 1
plt.show()

plt.figure(figsize=(14,22))
i = 1
for e in os.listdir(test_path):
    img = load_img((test_path + e +'/'+ os.listdir(test_path + e)[5]))
    plt.subplot(1,7,i)
    plt.imshow(img)
    plt.title(e)
    plt.axis('off')
    i += 1
plt.show()

train_data_gen = ImageDataGenerator(rescale=1./255,
                                   zoom_range=0.3,
                                   horizontal_flip=True)

train_set = train_data_gen.flow_from_directory(train_path, 
                                                 batch_size=64, 
                                                 target_size=(48,48), 
                                                 shuffle=True, 
                                                 color_mode='grayscale', 
                                                 class_mode='categorical')

test_data_gen = ImageDataGenerator(rescale=1./255)

test_set = test_data_gen.flow_from_directory(test_path, 
                                                 batch_size=64, 
                                                 target_size=(48,48), 
                                                 shuffle=True, 
                                                 color_mode='grayscale', 
                                                 class_mode='categorical')

train_set.class_indices

test_set.class_indices

def model_cnn(input_size, classes):
    model = tf.keras.models.Sequential()   

    model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape =input_size))
    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(2, 2))
    model.add(Dropout(0.25))

    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01)))
    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))
    
    model.add(Conv2D(512, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01)))
    model.add(Conv2D(512, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01)))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))
    
    model.add(Flatten())
    model.add(Dense(1024, activation='relu'))
    model.add(Dropout(0.5))
    
    model.add(Dense(classes, activation='softmax'))

    #Compliling the model
    model.compile(optimizer=Adam(lr=0.0001, decay=1e-6), 
                  loss='categorical_crossentropy', 
                  metrics=['accuracy'])
    return model

row, col = 48, 48
classes = 2
FER_model = model_cnn((row,col,1), classes)

hist = FER_model.fit(x=train_set,
                 validation_data=test_set,
                 epochs=20)

"""### Visualizing Model Accuracy"""

plt.figure(figsize=(14,5))
plt.subplot(1,2,2)
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['train', 'test'], loc='upper left')

plt.subplot(1,2,1)
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

train_loss, train_accu = FER_model.evaluate(train_set)
test_loss, test_accu = FER_model.evaluate(test_set)
print("Train accuracy = {:.2f} , Validation accuracy = {:.2f}".format(train_accu*100, test_accu*100))

FER_model.save_weights('fer_model_best.h5')
